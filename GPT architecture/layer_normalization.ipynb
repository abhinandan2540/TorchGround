{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432da9a5",
   "metadata": {},
   "source": [
    "**Normalizing activations with layer normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b5c180",
   "metadata": {},
   "source": [
    "Layer Normalization is typically applied before and after the multi-head attention module and before the final output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2e223",
   "metadata": {},
   "source": [
    "*The main idea behind layer normalization is to adjust the activations (outputs) of a neural network layer to have a mean of 0 and a variance of 1, also known as unit variance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7f4c95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(123)\n",
    "batch_example=torch.randn(2,5) # 2 input example\n",
    "# print(batch_example)\n",
    "\n",
    "layer=nn.Sequential(nn.Linear(5,6), nn.ReLU()) # 5 input, 6 output\n",
    "out=layer(batch_example)\n",
    "print(out)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3d771e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# before applying layer normalization\n",
    "mean=out.mean(dim=-1, keepdim=True)\n",
    "print(mean) # mean of 1st and 2nd input row\n",
    "\n",
    "var=out.var(dim=-1, keepdim=True)\n",
    "print(var) # variance of 1st, 2nd input row\n",
    "\n",
    "# dim=1,-1 calculates the mean across column dimension \n",
    "# dim=0 calculates the mean across row dimension\n",
    "\n",
    "# for 3D \n",
    "# [batch_size, num_tokens, embedding_size]\n",
    "# there we'll use dim=-1 instead of dim=1,2 etc\n",
    "# -1 refers to the tensors last dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9403709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying z score normalization\n",
    "out_norm=(out-mean)/torch.sqrt(var)\n",
    "mean=out_norm.mean(dim=-1, keepdim=True)\n",
    "var=out_norm.var(dim=-1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57d604f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[9.9341e-09],\n",
      "        [0.0000e+00]], grad_fn=<MeanBackward1>)\n",
      "tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out_norm)\n",
    "print(mean)\n",
    "print(var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99c19589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000],\n",
      "        [0.0000]], grad_fn=<MeanBackward1>)\n",
      "tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e8a0e1",
   "metadata": {},
   "source": [
    "**A layer normalization class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "648170a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps=1e-5\n",
    "        self.scale=nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift=nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1, keepdim=True)\n",
    "        var=x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x=(x-mean)/torch.sqrt(var+self.eps)\n",
    "        return self.scale*norm_x+self.shift\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e632e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0000],\n",
      "        [ 0.0000]], grad_fn=<MeanBackward1>)\n",
      "tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# trying LayerNorm\n",
    "ln=LayerNorm(emb_dim=5)\n",
    "out_ln=ln(batch_example)\n",
    "\n",
    "mean=out_ln.mean(dim=-1, keepdim=True)\n",
    "var=out_ln.var(dim=-1, keepdim=True, unbiased=False)\n",
    "\n",
    "print(mean)\n",
    "print(var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34a2507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
