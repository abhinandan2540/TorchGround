{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26a11395",
   "metadata": {},
   "source": [
    "There are 4 steps involve in data preprocessing for LLMs\n",
    "1. converting text into tokens\n",
    "2. tokens into token IDs\n",
    "3. token IDs into dataloader (token IDs loader)\n",
    "4. token IDs (loader) into embedding vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67732890",
   "metadata": {},
   "source": [
    "**converting text into tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed40178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n",
      "20479\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open ('the-verdict.txt' ,'r') as f:\n",
    "    raw_text=f.read()\n",
    "print(raw_text[:99])\n",
    "print(len(raw_text)) # total number of characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa618239",
   "metadata": {},
   "source": [
    "for building LLM, tokens plays a crucial role in data preprocessing\n",
    "in this example, we'll use python's `regular expression` syntax for that.\n",
    "\n",
    "for real life usage, when building an LLM, there exists manypre-built tokenizer for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85e953cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'World.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text='Hello, World. This, is a test.'\n",
    "result=re.split(r'(\\s)', text) # at re.split (patten for splitting, string for splitting)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cae6991e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'World', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result=re.split(r'([,.]|\\s)', text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d0d48e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'World', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "# for removing existing whitespaces from the result\n",
    "result=[item for item in result if item.strip()] \n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed254a0",
   "metadata": {},
   "source": [
    "tokenization level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ff6dddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Is', ' ', 'this', '--', '', ' ', 'a', ' ', 'test', '?', '']\n"
     ]
    }
   ],
   "source": [
    "text='Hello, world. Is this-- a test?'\n",
    "result=re.split(r'([,.?]|--|\\s)',text) # way we can generalize the tokenization pattern\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68f60f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb70913e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed=re.split(r'([.,:;?_!\"()\\']|--|\\s)',raw_text)\n",
    "preprocessed=[item for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed)) # number of tokens in the text (without whitespaces)\n",
    "print(preprocessed[:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87522730",
   "metadata": {},
   "source": [
    "**converting tokens into token IDs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c624ba6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n",
      "1130\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))\n",
    "all_words=sorted(set(preprocessed)) # only take the unique & represent into alphabatically order\n",
    "vocab_size=len(all_words)\n",
    "print(vocab_size)\n",
    "# print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f43814f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, '\"': 1, \"'\": 2, '(': 3, ')': 4, ',': 5, '--': 6, '.': 7, ':': 8, ';': 9, '?': 10, 'A': 11, 'Ah': 12, 'Among': 13, 'And': 14, 'Are': 15, 'Arrt': 16, 'As': 17, 'At': 18, 'Be': 19, 'Begin': 20, 'Burlington': 21, 'But': 22, 'By': 23, 'Carlo': 24, 'Chicago': 25, 'Claude': 26, 'Come': 27, 'Croft': 28, 'Destroyed': 29, 'Devonshire': 30, 'Don': 31, 'Dubarry': 32, 'Emperors': 33, 'Florence': 34, 'For': 35, 'Gallery': 36, 'Gideon': 37, 'Gisburn': 38, 'Gisburns': 39, 'Grafton': 40, 'Greek': 41, 'Grindle': 42, 'Grindles': 43, 'HAD': 44, 'Had': 45, 'Hang': 46, 'Has': 47, 'He': 48, 'Her': 49, 'Hermia': 50, 'His': 51, 'How': 52, 'I': 53, 'If': 54, 'In': 55, 'It': 56, 'Jack': 57, 'Jove': 58, 'Just': 59, 'Lord': 60, 'Made': 61, 'Miss': 62, 'Money': 63, 'Monte': 64, 'Moon-dancers': 65, 'Mr': 66, 'Mrs': 67, 'My': 68, 'Never': 69, 'No': 70, 'Now': 71, 'Nutley': 72, 'Of': 73, 'Oh': 74, 'On': 75, 'Once': 76, 'Only': 77, 'Or': 78, 'Perhaps': 79, 'Poor': 80, 'Professional': 81, 'Renaissance': 82, 'Rickham': 83, 'Riviera': 84, 'Rome': 85, 'Russian': 86, 'Sevres': 87, 'She': 88, 'Stroud': 89, 'Strouds': 90, 'Suddenly': 91, 'That': 92, 'The': 93, 'Then': 94, 'There': 95, 'They': 96, 'This': 97, 'Those': 98, 'Though': 99, 'Thwing': 100, 'Thwings': 101, 'To': 102, 'Usually': 103, 'Venetian': 104, 'Victor': 105, 'Was': 106, 'We': 107, 'Well': 108, 'What': 109, 'When': 110, 'Why': 111, 'Yes': 112, 'You': 113, '_': 114, 'a': 115, 'abdication': 116, 'able': 117, 'about': 118, 'above': 119, 'abruptly': 120, 'absolute': 121, 'absorbed': 122, 'absurdity': 123, 'academic': 124, 'accuse': 125, 'accustomed': 126, 'across': 127, 'activity': 128, 'add': 129, 'added': 130, 'admirers': 131, 'adopted': 132, 'adulation': 133, 'advance': 134, 'aesthetic': 135, 'affect': 136, 'afraid': 137, 'after': 138, 'afterward': 139, 'again': 140, 'ago': 141, 'ah': 142, 'air': 143, 'alive': 144, 'all': 145, 'almost': 146, 'alone': 147, 'along': 148, 'always': 149, 'am': 150, 'amazement': 151, 'amid': 152, 'among': 153, 'amplest': 154, 'amusing': 155, 'an': 156, 'and': 157, 'another': 158, 'answer': 159, 'answered': 160, 'any': 161, 'anything': 162, 'anywhere': 163, 'apparent': 164, 'apparently': 165, 'appearance': 166, 'appeared': 167, 'appointed': 168, 'are': 169, 'arm': 170, 'arm-chair': 171, 'arm-chairs': 172, 'arms': 173, 'art': 174, 'articles': 175, 'artist': 176, 'as': 177, 'aside': 178, 'asked': 179, 'at': 180, 'atmosphere': 181, 'atom': 182, 'attack': 183, 'attention': 184, 'attitude': 185, 'audacities': 186, 'away': 187, 'awful': 188, 'axioms': 189, 'azaleas': 190, 'back': 191, 'background': 192, 'balance': 193, 'balancing': 194, 'balustraded': 195, 'basking': 196, 'bath-rooms': 197, 'be': 198, 'beaming': 199, 'bean-stalk': 200, 'bear': 201, 'beard': 202, 'beauty': 203, 'became': 204, 'because': 205, 'becoming': 206, 'bed': 207, 'been': 208, 'before': 209, 'began': 210, 'begun': 211, 'behind': 212, 'being': 213, 'believed': 214, 'beneath': 215, 'bespoke': 216, 'better': 217, 'between': 218, 'big': 219, 'bits': 220, 'bitterness': 221, 'blocked': 222, 'born': 223, 'borne': 224, 'boudoir': 225, 'bravura': 226, 'break': 227, 'breaking': 228, 'breathing': 229, 'bric-a-brac': 230, 'briefly': 231, 'brings': 232, 'bronzes': 233, 'brought': 234, 'brown': 235, 'brush': 236, 'bull': 237, 'business': 238, 'but': 239, 'buying': 240, 'by': 241, 'called': 242, 'came': 243, 'can': 244, 'canvas': 245, 'canvases': 246, 'cards': 247, 'care': 248, 'career': 249, 'caught': 250, 'central': 251, 'chair': 252, 'chap': 253, 'characteristic': 254, 'charming': 255, 'cheap': 256, 'check': 257, 'cheeks': 258, 'chest': 259, 'chimney-piece': 260, 'chucked': 261, 'cigar': 262, 'cigarette': 263, 'cigars': 264, 'circulation': 265, 'circumstance': 266, 'circus-clown': 267, 'claimed': 268, 'clasping': 269, 'clear': 270, 'cleverer': 271, 'close': 272, 'clue': 273, 'coat': 274, 'collapsed': 275, 'colour': 276, 'come': 277, 'comfortable': 278, 'coming': 279, 'companion': 280, 'compared': 281, 'complex': 282, 'confident': 283, 'congesting': 284, 'conjugal': 285, 'constraint': 286, 'consummate': 287, 'contended': 288, 'continued': 289, 'corner': 290, 'corrected': 291, 'could': 292, 'couldn': 293, 'count': 294, 'countenance': 295, 'couple': 296, 'course': 297, 'covered': 298, 'craft': 299, 'cried': 300, 'crossed': 301, 'crowned': 302, 'crumbled': 303, 'cry': 304, 'cured': 305, 'curiosity': 306, 'curious': 307, 'current': 308, 'curtains': 309, 'd': 310, 'dabble': 311, 'damask': 312, 'dark': 313, 'dashed': 314, 'day': 315, 'days': 316, 'dead': 317, 'deadening': 318, 'dear': 319, 'deep': 320, 'deerhound': 321, 'degree': 322, 'delicate': 323, 'demand': 324, 'denied': 325, 'deploring': 326, 'deprecating': 327, 'deprecatingly': 328, 'desire': 329, 'destroyed': 330, 'destruction': 331, 'desultory': 332, 'detail': 333, 'diagnosis': 334, 'did': 335, 'didn': 336, 'died': 337, 'dim': 338, 'dimmest': 339, 'dingy': 340, 'dining-room': 341, 'disarming': 342, 'discovery': 343, 'discrimination': 344, 'discussion': 345, 'disdain': 346, 'disdained': 347, 'disease': 348, 'disguised': 349, 'display': 350, 'dissatisfied': 351, 'distinguished': 352, 'distract': 353, 'divert': 354, 'do': 355, 'doesn': 356, 'doing': 357, 'domestic': 358, 'don': 359, 'done': 360, 'donkey': 361, 'down': 362, 'dozen': 363, 'dragged': 364, 'drawing-room': 365, 'drawing-rooms': 366, 'drawn': 367, 'dress-closets': 368, 'drew': 369, 'dropped': 370, 'each': 371, 'earth': 372, 'ease': 373, 'easel': 374, 'easy': 375, 'echoed': 376, 'economy': 377, 'effect': 378, 'effects': 379, 'efforts': 380, 'egregious': 381, 'eighteenth-century': 382, 'elbow': 383, 'elegant': 384, 'else': 385, 'embarrassed': 386, 'enabled': 387, 'end': 388, 'endless': 389, 'enjoy': 390, 'enlightenment': 391, 'enough': 392, 'ensuing': 393, 'equally': 394, 'equanimity': 395, 'escape': 396, 'established': 397, 'etching': 398, 'even': 399, 'event': 400, 'ever': 401, 'everlasting': 402, 'every': 403, 'exasperated': 404, 'except': 405, 'excuse': 406, 'excusing': 407, 'existed': 408, 'expected': 409, 'exquisite': 410, 'exquisitely': 411, 'extenuation': 412, 'exterminating': 413, 'extracting': 414, 'eye': 415, 'eyebrows': 416, 'eyes': 417, 'face': 418, 'faces': 419, 'fact': 420, 'faded': 421, 'failed': 422, 'failure': 423, 'fair': 424, 'faith': 425, 'false': 426, 'familiar': 427, 'famille-verte': 428, 'fancy': 429, 'fashionable': 430, 'fate': 431, 'feather': 432, 'feet': 433, 'fell': 434, 'fellow': 435, 'felt': 436, 'few': 437, 'fewer': 438, 'finality': 439, 'find': 440, 'fingers': 441, 'first': 442, 'fit': 443, 'fitting': 444, 'five': 445, 'flash': 446, 'flashed': 447, 'florid': 448, 'flowers': 449, 'fluently': 450, 'flung': 451, 'follow': 452, 'followed': 453, 'fond': 454, 'footstep': 455, 'for': 456, 'forced': 457, 'forcing': 458, 'forehead': 459, 'foreign': 460, 'foreseen': 461, 'forgive': 462, 'forgotten': 463, 'form': 464, 'formed': 465, 'forming': 466, 'forward': 467, 'fostered': 468, 'found': 469, 'foundations': 470, 'fragment': 471, 'fragments': 472, 'frame': 473, 'frames': 474, 'frequently': 475, 'friend': 476, 'from': 477, 'full': 478, 'fullest': 479, 'furiously': 480, 'furrowed': 481, 'garlanded': 482, 'garlands': 483, 'gave': 484, 'genial': 485, 'genius': 486, 'gesture': 487, 'get': 488, 'getting': 489, 'give': 490, 'given': 491, 'glad': 492, 'glanced': 493, 'glimpse': 494, 'gloried': 495, 'glory': 496, 'go': 497, 'going': 498, 'gone': 499, 'good': 500, 'good-breeding': 501, 'good-humoured': 502, 'got': 503, 'grace': 504, 'gradually': 505, 'gray': 506, 'grayish': 507, 'great': 508, 'greatest': 509, 'greatness': 510, 'grew': 511, 'groping': 512, 'growing': 513, 'had': 514, 'hadn': 515, 'hair': 516, 'half': 517, 'half-light': 518, 'half-mechanically': 519, 'hall': 520, 'hand': 521, 'hands': 522, 'handsome': 523, 'hanging': 524, 'happen': 525, 'happened': 526, 'hard': 527, 'hardly': 528, 'has': 529, 'have': 530, 'haven': 531, 'having': 532, 'he': 533, 'head': 534, 'hear': 535, 'heard': 536, 'heart': 537, 'height': 538, 'her': 539, 'here': 540, 'hermit': 541, 'herself': 542, 'hesitations': 543, 'hide': 544, 'high': 545, 'him': 546, 'himself': 547, 'hint': 548, 'his': 549, 'history': 550, 'holding': 551, 'home': 552, 'honour': 553, 'hooded': 554, 'hostess': 555, 'hot-house': 556, 'hour': 557, 'hours': 558, 'house': 559, 'how': 560, 'hung': 561, 'husband': 562, 'idea': 563, 'idle': 564, 'idling': 565, 'if': 566, 'immediately': 567, 'in': 568, 'incense': 569, 'indifferent': 570, 'inevitable': 571, 'inevitably': 572, 'inflexible': 573, 'insensible': 574, 'insignificant': 575, 'instinctively': 576, 'instructive': 577, 'interesting': 578, 'into': 579, 'ironic': 580, 'irony': 581, 'irrelevance': 582, 'irrevocable': 583, 'is': 584, 'it': 585, 'its': 586, 'itself': 587, 'jardiniere': 588, 'jealousy': 589, 'just': 590, 'keep': 591, 'kept': 592, 'kind': 593, 'knees': 594, 'knew': 595, 'know': 596, 'known': 597, 'laid': 598, 'lair': 599, 'landing': 600, 'language': 601, 'last': 602, 'late': 603, 'later': 604, 'latter': 605, 'laugh': 606, 'laughed': 607, 'lay': 608, 'leading': 609, 'lean': 610, 'learned': 611, 'least': 612, 'leathery': 613, 'leave': 614, 'led': 615, 'left': 616, 'leisure': 617, 'lends': 618, 'lent': 619, 'let': 620, 'lies': 621, 'life': 622, 'life-likeness': 623, 'lift': 624, 'lifted': 625, 'light': 626, 'lightly': 627, 'like': 628, 'liked': 629, 'line': 630, 'lines': 631, 'lingered': 632, 'lips': 633, 'lit': 634, 'little': 635, 'live': 636, 'll': 637, 'loathing': 638, 'long': 639, 'longed': 640, 'longer': 641, 'look': 642, 'looked': 643, 'looking': 644, 'lose': 645, 'loss': 646, 'lounging': 647, 'lovely': 648, 'lucky': 649, 'lump': 650, 'luncheon-table': 651, 'luxury': 652, 'lying': 653, 'made': 654, 'make': 655, 'man': 656, 'manage': 657, 'managed': 658, 'mantel-piece': 659, 'marble': 660, 'married': 661, 'may': 662, 'me': 663, 'meant': 664, 'mediocrity': 665, 'medium': 666, 'mentioned': 667, 'mere': 668, 'merely': 669, 'met': 670, 'might': 671, 'mighty': 672, 'millionaire': 673, 'mine': 674, 'minute': 675, 'minutes': 676, 'mirrors': 677, 'modest': 678, 'modesty': 679, 'moment': 680, 'money': 681, 'monumental': 682, 'mood': 683, 'morbidly': 684, 'more': 685, 'most': 686, 'mourn': 687, 'mourned': 688, 'moustache': 689, 'moved': 690, 'much': 691, 'muddling': 692, 'multiplied': 693, 'murmur': 694, 'muscles': 695, 'must': 696, 'my': 697, 'myself': 698, 'mysterious': 699, 'naive': 700, 'near': 701, 'nearly': 702, 'negatived': 703, 'nervous': 704, 'nervousness': 705, 'neutral': 706, 'never': 707, 'next': 708, 'no': 709, 'none': 710, 'not': 711, 'note': 712, 'nothing': 713, 'now': 714, 'nymphs': 715, 'oak': 716, 'obituary': 717, 'object': 718, 'objects': 719, 'occurred': 720, 'oddly': 721, 'of': 722, 'off': 723, 'often': 724, 'oh': 725, 'old': 726, 'on': 727, 'once': 728, 'one': 729, 'ones': 730, 'only': 731, 'onto': 732, 'open': 733, 'or': 734, 'other': 735, 'our': 736, 'ourselves': 737, 'out': 738, 'outline': 739, 'oval': 740, 'over': 741, 'own': 742, 'packed': 743, 'paid': 744, 'paint': 745, 'painted': 746, 'painter': 747, 'painting': 748, 'pale': 749, 'paled': 750, 'palm-trees': 751, 'panel': 752, 'panelling': 753, 'pardonable': 754, 'pardoned': 755, 'part': 756, 'passages': 757, 'passing': 758, 'past': 759, 'pastels': 760, 'pathos': 761, 'patient': 762, 'people': 763, 'perceptible': 764, 'perfect': 765, 'persistence': 766, 'persuasively': 767, 'phrase': 768, 'picture': 769, 'pictures': 770, 'pines': 771, 'pink': 772, 'place': 773, 'placed': 774, 'plain': 775, 'platitudes': 776, 'pleased': 777, 'pockets': 778, 'point': 779, 'poised': 780, 'poor': 781, 'portrait': 782, 'posing': 783, 'possessed': 784, 'poverty': 785, 'predicted': 786, 'preliminary': 787, 'presenting': 788, 'prestidigitation': 789, 'pretty': 790, 'previous': 791, 'price': 792, 'pride': 793, 'princely': 794, 'prism': 795, 'problem': 796, 'proclaiming': 797, 'prodigious': 798, 'profusion': 799, 'protest': 800, 'prove': 801, 'public': 802, 'purblind': 803, 'purely': 804, 'pushed': 805, 'put': 806, 'qualities': 807, 'quality': 808, 'queerly': 809, 'question': 810, 'quickly': 811, 'quietly': 812, 'quite': 813, 'quote': 814, 'rain': 815, 'raised': 816, 'random': 817, 'rather': 818, 're': 819, 'real': 820, 'really': 821, 'reared': 822, 'reason': 823, 'reassurance': 824, 'recovering': 825, 'recreated': 826, 'reflected': 827, 'reflection': 828, 'regrets': 829, 'relatively': 830, 'remained': 831, 'remember': 832, 'reminded': 833, 'repeating': 834, 'represented': 835, 'reproduction': 836, 'resented': 837, 'resolve': 838, 'resources': 839, 'rest': 840, 'rich': 841, 'ridiculous': 842, 'robbed': 843, 'romantic': 844, 'room': 845, 'rose': 846, 'rs': 847, 'rule': 848, 'run': 849, 's': 850, 'said': 851, 'same': 852, 'satisfaction': 853, 'savour': 854, 'saw': 855, 'say': 856, 'saying': 857, 'says': 858, 'scorn': 859, 'scornful': 860, 'secret': 861, 'see': 862, 'seemed': 863, 'seen': 864, 'self-confident': 865, 'send': 866, 'sensation': 867, 'sensitive': 868, 'sent': 869, 'serious': 870, 'set': 871, 'sex': 872, 'shade': 873, 'shaking': 874, 'shall': 875, 'she': 876, 'shirked': 877, 'short': 878, 'should': 879, 'shoulder': 880, 'shoulders': 881, 'show': 882, 'showed': 883, 'showy': 884, 'shrug': 885, 'shrugged': 886, 'sight': 887, 'sign': 888, 'silent': 889, 'silver': 890, 'similar': 891, 'simpleton': 892, 'simplifications': 893, 'simply': 894, 'since': 895, 'single': 896, 'sitter': 897, 'sitters': 898, 'sketch': 899, 'skill': 900, 'slight': 901, 'slightly': 902, 'slowly': 903, 'small': 904, 'smile': 905, 'smiling': 906, 'sneer': 907, 'so': 908, 'solace': 909, 'some': 910, 'somebody': 911, 'something': 912, 'spacious': 913, 'spaniel': 914, 'speaking-tubes': 915, 'speculations': 916, 'spite': 917, 'splash': 918, 'square': 919, 'stairs': 920, 'stammer': 921, 'stand': 922, 'standing': 923, 'started': 924, 'stay': 925, 'still': 926, 'stocked': 927, 'stood': 928, 'stopped': 929, 'stopping': 930, 'straddling': 931, 'straight': 932, 'strain': 933, 'straining': 934, 'strange': 935, 'straw': 936, 'stream': 937, 'stroke': 938, 'strokes': 939, 'strolled': 940, 'strongest': 941, 'strongly': 942, 'struck': 943, 'studio': 944, 'stuff': 945, 'subject': 946, 'substantial': 947, 'suburban': 948, 'such': 949, 'suddenly': 950, 'suffered': 951, 'sugar': 952, 'suggested': 953, 'sunburn': 954, 'sunburnt': 955, 'sunlit': 956, 'superb': 957, 'sure': 958, 'surest': 959, 'surface': 960, 'surprise': 961, 'surprised': 962, 'surrounded': 963, 'suspected': 964, 'sweetly': 965, 'sweetness': 966, 'swelling': 967, 'swept': 968, 'swum': 969, 't': 970, 'table': 971, 'take': 972, 'taken': 973, 'talking': 974, 'tea': 975, 'tears': 976, 'technicalities': 977, 'technique': 978, 'tell': 979, 'tells': 980, 'tempting': 981, 'terra-cotta': 982, 'terrace': 983, 'terraces': 984, 'terribly': 985, 'than': 986, 'that': 987, 'the': 988, 'their': 989, 'them': 990, 'then': 991, 'there': 992, 'therefore': 993, 'they': 994, 'thin': 995, 'thing': 996, 'things': 997, 'think': 998, 'this': 999, 'thither': 1000, 'those': 1001, 'though': 1002, 'thought': 1003, 'three': 1004, 'threshold': 1005, 'threw': 1006, 'through': 1007, 'throwing': 1008, 'tie': 1009, 'till': 1010, 'time': 1011, 'timorously': 1012, 'tinge': 1013, 'tips': 1014, 'tired': 1015, 'to': 1016, 'told': 1017, 'tone': 1018, 'tones': 1019, 'too': 1020, 'took': 1021, 'tottering': 1022, 'touched': 1023, 'toward': 1024, 'trace': 1025, 'trade': 1026, 'transmute': 1027, 'traps': 1028, 'travelled': 1029, 'tribute': 1030, 'tributes': 1031, 'tricks': 1032, 'tried': 1033, 'trouser-presses': 1034, 'true': 1035, 'truth': 1036, 'turned': 1037, 'twenty': 1038, 'twenty-four': 1039, 'twice': 1040, 'twirling': 1041, 'unaccountable': 1042, 'uncertain': 1043, 'under': 1044, 'underlay': 1045, 'underneath': 1046, 'understand': 1047, 'unexpected': 1048, 'untouched': 1049, 'unusual': 1050, 'up': 1051, 'up-stream': 1052, 'upon': 1053, 'upset': 1054, 'upstairs': 1055, 'us': 1056, 'used': 1057, 'usual': 1058, 'value': 1059, 'varnishing': 1060, 'vases': 1061, 've': 1062, 'veins': 1063, 'velveteen': 1064, 'very': 1065, 'villa': 1066, 'vindicated': 1067, 'virtuosity': 1068, 'vista': 1069, 'vocation': 1070, 'voice': 1071, 'wall': 1072, 'wander': 1073, 'want': 1074, 'wanted': 1075, 'wants': 1076, 'was': 1077, 'wasn': 1078, 'watched': 1079, 'watching': 1080, 'water-colour': 1081, 'waves': 1082, 'way': 1083, 'weekly': 1084, 'weeks': 1085, 'welcome': 1086, 'went': 1087, 'were': 1088, 'what': 1089, 'when': 1090, 'whenever': 1091, 'where': 1092, 'which': 1093, 'while': 1094, 'white': 1095, 'white-panelled': 1096, 'who': 1097, 'whole': 1098, 'whom': 1099, 'why': 1100, 'wide': 1101, 'widow': 1102, 'wife': 1103, 'wild': 1104, 'wincing': 1105, 'window-curtains': 1106, 'wish': 1107, 'with': 1108, 'without': 1109, 'wits': 1110, 'woman': 1111, 'women': 1112, 'won': 1113, 'wonder': 1114, 'wondered': 1115, 'word': 1116, 'work': 1117, 'working': 1118, 'worth': 1119, 'would': 1120, 'wouldn': 1121, 'year': 1122, 'years': 1123, 'yellow': 1124, 'yet': 1125, 'you': 1126, 'younger': 1127, 'your': 1128, 'yourself': 1129}\n"
     ]
    }
   ],
   "source": [
    "# vocab building\n",
    "char_to_indx={char:i for i,char in enumerate(all_words)}\n",
    "indx_to_char={i:char for i,char in enumerate(all_words)}\n",
    "\n",
    "# vocab needs to be make at first\n",
    "vocab={token:indx for indx, token in enumerate(all_words)}\n",
    "print(vocab)\n",
    "# vocab is a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abec9c6",
   "metadata": {},
   "source": [
    "*Implementing a simple text tokenizer* Type 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2e7252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int=vocab \n",
    "        self.int_to_str={i:s for s,i in vocab.items()}\n",
    "        \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([.,:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids=[self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text=\" \".join([self.int_to_str[i] for i in ids])\n",
    "        text=re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
    "        return text\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83ef0f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "tokenizer=SimpleTokenizerV1(vocab=vocab)\n",
    "text=\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable \n",
    "pride.\"\"\"\n",
    "\n",
    "ids=tokenizer.encode(text=text)\n",
    "print(ids) # corresponding token ids from the vocab\n",
    "\n",
    "print(tokenizer.decode(ids=ids)) # from ids to string\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca9c4e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# what if we apply sample tokenizer into an unseen data\n",
    "\n",
    "# text='Hello, do you like tea?'\n",
    "# print(tokenizer.encode(text=text))\n",
    "\n",
    "# that's why we need large amount of data when working with LLM's\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa3bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens=sorted(list(set(preprocessed)))\n",
    "# all_tokens.extend([\"<|endoftext|>\",\"<|unk|>\"])\n",
    "\n",
    "# <|endoftext|>, <|unk|> these are added into preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bef53272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "vocab={token:indx for indx,token in enumerate(all_tokens)}\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edde4ee3",
   "metadata": {},
   "source": [
    "Adding special context tokens \n",
    "\n",
    "*simple tokenizer for handeling unknown words* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3fc9f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "\n",
    "    def encoder(self, text):\n",
    "        preprocessed = re.split(r'([.,:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decoder(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4f0ec66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "# now applying the same concept like GPT training for LLMs\n",
    "\n",
    "text1=\"Hello, do you like tea?\"\n",
    "text2=\"In the sunlit terraces of the palace.\"\n",
    "text=\" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b257abe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer2=SampleTokenizerV2(vocab=vocab)\n",
    "print(tokenizer2.encoder(text=text))\n",
    "# print(\"Encoder output (ids):\", tokenizer2.encoder(text=text))\n",
    "# 1128 contains <|endoftext|>, so it dosen't throwing any keyError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3344dd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "# print(\"Has <|unk|> in vocab:\", \"<|unk|>\" in vocab, \"id=\", vocab.get(\"<|unk|>\"))\n",
    "print(tokenizer2.decoder(tokenizer2.encoder(text=text)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bb8d76",
   "metadata": {},
   "source": [
    "Byte Pair Encoding\n",
    "\n",
    "*GPT mostly uses BPE for converting tokens into IDs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e7d7ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12.0\n"
     ]
    }
   ],
   "source": [
    "# from importlib.metadata import version\n",
    "# import tiktoken\n",
    "# print(version(\"tiktoken\"))\n",
    "\n",
    "import tiktoken\n",
    "print(tiktoken.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51ed7d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 5633, 220, 50256, 554, 262, 4252, 18250, 286, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer=tiktoken.get_encoding(\"gpt2\") # encoding of GPT 2\n",
    "\n",
    "text=\"Hello, do you like tea ? <|endoftext|> In the sunlit of terraces of someunknownPlace.\"\n",
    "integers=tokenizer.encode(text=text,allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a61a987e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea ? <|endoftext|> In the sunlit of terraces of someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "# decoing the encoding IDs\n",
    "strings=tokenizer.decode(integers)\n",
    "print(strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41fcbf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "exe_text=\"Akwirw ier\"\n",
    "ids=tokenizer.encode(text=exe_text,allowed_special={\"<|endoftext|>\"})\n",
    "print(ids)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f9d1869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "ids_to_str=tokenizer.decode(ids)\n",
    "print(ids_to_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ca3754",
   "metadata": {},
   "source": [
    "*data sampling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89d43fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "with open(\"the-verdict.txt\",'r',encoding='utf-8') as f:\n",
    "    raw_text=f.read()\n",
    "\n",
    "enc_text=tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26373715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a little exercise, \n",
    "enc_samples=enc_text[50:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f3ce91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290, 4920, 2241, 287]\n",
      "[4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size=4 # determine how many tokens are included in the input\n",
    "x=enc_samples[:context_size]\n",
    "y=enc_samples[1:context_size+1]\n",
    "print(x)\n",
    "print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3db1b085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and --->  established\n",
      " and established --->  himself\n",
      " and established himself --->  in\n",
      " and established himself in --->  a\n"
     ]
    }
   ],
   "source": [
    "# a sliding window appraoch for better seeing\n",
    "# context : input\n",
    "# desired : target\n",
    "\n",
    "for i in range(1,context_size+1):\n",
    "    context=enc_samples[:i]\n",
    "    desired=enc_samples[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"--->\", tokenizer.decode([desired]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58080fb4",
   "metadata": {},
   "source": [
    "**converting token IDs into dataloader**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c96340",
   "metadata": {},
   "source": [
    "*before turing tokens into embeddings, we need to make an efficient data loader that iterates over the input dataset and returns the inputs and targets as pytorch tensors, whcih can be thought of as multidimentional arrays*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4e0e4c",
   "metadata": {},
   "source": [
    "a dataset for batched inputs and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2247302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.input_ids=[]\n",
    "        self.target_ids=[]\n",
    "\n",
    "        token_ids=tokenizer.encode(text) # tokenize the entire text\n",
    "\n",
    "        # sliding window approach used here\n",
    "        for i in range(0, len(token_ids)-max_length, stride):\n",
    "            input_chunk=token_ids[i:i+max_length]\n",
    "            target_chunk=token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003091f1",
   "metadata": {},
   "source": [
    "creating a dataloader to generate batches with input-wth pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ab39861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset=GPTDatasetV1(text=txt,tokenizer=tokenizer,max_length=max_length,stride=stride)\n",
    "    dataloader=DataLoader(dataset=dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd7a2c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   40,   367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138,\n",
      "           257,  7026, 15632,   438,  2016,   257,   922,  5891,  1576,   438,\n",
      "           568,   340,   373,   645,  1049,  5975,   284,   502,   284,  3285,\n",
      "           326,    11,   287,   262,  6001,   286,   465, 13476,    11,   339,\n",
      "           550,  5710,   465, 12036,    11,  6405,   257,  5527, 27075,    11,\n",
      "           290,  4920,  2241,   287,   257,  4489,    64,   319,   262, 34686,\n",
      "         41976,    13,   357, 10915,   314,  2138,  1807,   340,   561,   423,\n",
      "           587, 10598,   393, 28537,  2014,   198,   198,     1,   464,  6001,\n",
      "           286,   465, 13476,     1,   438,  5562,   373,   644,   262,  1466,\n",
      "          1444,   340,    13,   314,   460,  3285,  9074,    13, 46606,   536,\n",
      "          5469,   438, 14363,   938,  4842,  1650,   353,   438,  2934,   489,\n",
      "          3255,   465, 48422,   540,   450,    67,  3299,    13,   366,  5189,\n",
      "          1781,   340,   338,  1016,   284,  3758,   262,  1988,   286,   616,\n",
      "          4286,   705,  1014,   510,    26,   475,   314,   836,   470,   892,\n",
      "           286,   326,    11,  1770,    13,  8759,  2763,   438,  1169,  2994,\n",
      "           284,   943, 17034,   318,   477,   314,   892,   286,   526,   383,\n",
      "          1573,    11,   319,  9074,    13,   536,  5469,   338, 11914,    11,\n",
      "         33096,   663,  4808,  3808,    62,   355,   996,   484,   547, 12548,\n",
      "           287,   281, 13079,   410, 12523,   286, 22353,    13,   843,   340,\n",
      "           373,   407,   691,   262,  9074,    13,   536, 48819,   508, 25722,\n",
      "           276,    13, 11161,   407,   262, 40123, 18113,   544,  9325,   701,\n",
      "            11,   379,   262,   938,   402,  1617,   261, 12917,   905,    11,\n",
      "          5025,   502,   878,   402,   271, 10899,   338,   366, 31640,    12,\n",
      "            67, 20811,     1,   284,   910,    11,   351, 10953,   287,   607,\n",
      "          2951,    25,   366,  1135,  2236,   407,   804,  2402,   663,   588,\n",
      "           757, 13984,   198,   198,  5779, 28112]]), tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138,   257,\n",
      "          7026, 15632,   438,  2016,   257,   922,  5891,  1576,   438,   568,\n",
      "           340,   373,   645,  1049,  5975,   284,   502,   284,  3285,   326,\n",
      "            11,   287,   262,  6001,   286,   465, 13476,    11,   339,   550,\n",
      "          5710,   465, 12036,    11,  6405,   257,  5527, 27075,    11,   290,\n",
      "          4920,  2241,   287,   257,  4489,    64,   319,   262, 34686, 41976,\n",
      "            13,   357, 10915,   314,  2138,  1807,   340,   561,   423,   587,\n",
      "         10598,   393, 28537,  2014,   198,   198,     1,   464,  6001,   286,\n",
      "           465, 13476,     1,   438,  5562,   373,   644,   262,  1466,  1444,\n",
      "           340,    13,   314,   460,  3285,  9074,    13, 46606,   536,  5469,\n",
      "           438, 14363,   938,  4842,  1650,   353,   438,  2934,   489,  3255,\n",
      "           465, 48422,   540,   450,    67,  3299,    13,   366,  5189,  1781,\n",
      "           340,   338,  1016,   284,  3758,   262,  1988,   286,   616,  4286,\n",
      "           705,  1014,   510,    26,   475,   314,   836,   470,   892,   286,\n",
      "           326,    11,  1770,    13,  8759,  2763,   438,  1169,  2994,   284,\n",
      "           943, 17034,   318,   477,   314,   892,   286,   526,   383,  1573,\n",
      "            11,   319,  9074,    13,   536,  5469,   338, 11914,    11, 33096,\n",
      "           663,  4808,  3808,    62,   355,   996,   484,   547, 12548,   287,\n",
      "           281, 13079,   410, 12523,   286, 22353,    13,   843,   340,   373,\n",
      "           407,   691,   262,  9074,    13,   536, 48819,   508, 25722,   276,\n",
      "            13, 11161,   407,   262, 40123, 18113,   544,  9325,   701,    11,\n",
      "           379,   262,   938,   402,  1617,   261, 12917,   905,    11,  5025,\n",
      "           502,   878,   402,   271, 10899,   338,   366, 31640,    12,    67,\n",
      "         20811,     1,   284,   910,    11,   351, 10953,   287,   607,  2951,\n",
      "            25,   366,  1135,  2236,   407,   804,  2402,   663,   588,   757,\n",
      "         13984,   198,   198,  5779, 28112, 10197]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader=create_dataloader_v1(txt=raw_text,batch_size=1,stride=1,shuffle=False)\n",
    "\n",
    "# converting dataloader into python iterator\n",
    "data_iter=iter(dataloader)\n",
    "first_batch=next(data_iter)\n",
    "print(first_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64d21fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138,   257,\n",
      "          7026, 15632,   438,  2016,   257,   922,  5891,  1576,   438,   568,\n",
      "           340,   373,   645,  1049,  5975,   284,   502,   284,  3285,   326,\n",
      "            11,   287,   262,  6001,   286,   465, 13476,    11,   339,   550,\n",
      "          5710,   465, 12036,    11,  6405,   257,  5527, 27075,    11,   290,\n",
      "          4920,  2241,   287,   257,  4489,    64,   319,   262, 34686, 41976,\n",
      "            13,   357, 10915,   314,  2138,  1807,   340,   561,   423,   587,\n",
      "         10598,   393, 28537,  2014,   198,   198,     1,   464,  6001,   286,\n",
      "           465, 13476,     1,   438,  5562,   373,   644,   262,  1466,  1444,\n",
      "           340,    13,   314,   460,  3285,  9074,    13, 46606,   536,  5469,\n",
      "           438, 14363,   938,  4842,  1650,   353,   438,  2934,   489,  3255,\n",
      "           465, 48422,   540,   450,    67,  3299,    13,   366,  5189,  1781,\n",
      "           340,   338,  1016,   284,  3758,   262,  1988,   286,   616,  4286,\n",
      "           705,  1014,   510,    26,   475,   314,   836,   470,   892,   286,\n",
      "           326,    11,  1770,    13,  8759,  2763,   438,  1169,  2994,   284,\n",
      "           943, 17034,   318,   477,   314,   892,   286,   526,   383,  1573,\n",
      "            11,   319,  9074,    13,   536,  5469,   338, 11914,    11, 33096,\n",
      "           663,  4808,  3808,    62,   355,   996,   484,   547, 12548,   287,\n",
      "           281, 13079,   410, 12523,   286, 22353,    13,   843,   340,   373,\n",
      "           407,   691,   262,  9074,    13,   536, 48819,   508, 25722,   276,\n",
      "            13, 11161,   407,   262, 40123, 18113,   544,  9325,   701,    11,\n",
      "           379,   262,   938,   402,  1617,   261, 12917,   905,    11,  5025,\n",
      "           502,   878,   402,   271, 10899,   338,   366, 31640,    12,    67,\n",
      "         20811,     1,   284,   910,    11,   351, 10953,   287,   607,  2951,\n",
      "            25,   366,  1135,  2236,   407,   804,  2402,   663,   588,   757,\n",
      "         13984,   198,   198,  5779, 28112, 10197]]), tensor([[ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138,   257,  7026,\n",
      "         15632,   438,  2016,   257,   922,  5891,  1576,   438,   568,   340,\n",
      "           373,   645,  1049,  5975,   284,   502,   284,  3285,   326,    11,\n",
      "           287,   262,  6001,   286,   465, 13476,    11,   339,   550,  5710,\n",
      "           465, 12036,    11,  6405,   257,  5527, 27075,    11,   290,  4920,\n",
      "          2241,   287,   257,  4489,    64,   319,   262, 34686, 41976,    13,\n",
      "           357, 10915,   314,  2138,  1807,   340,   561,   423,   587, 10598,\n",
      "           393, 28537,  2014,   198,   198,     1,   464,  6001,   286,   465,\n",
      "         13476,     1,   438,  5562,   373,   644,   262,  1466,  1444,   340,\n",
      "            13,   314,   460,  3285,  9074,    13, 46606,   536,  5469,   438,\n",
      "         14363,   938,  4842,  1650,   353,   438,  2934,   489,  3255,   465,\n",
      "         48422,   540,   450,    67,  3299,    13,   366,  5189,  1781,   340,\n",
      "           338,  1016,   284,  3758,   262,  1988,   286,   616,  4286,   705,\n",
      "          1014,   510,    26,   475,   314,   836,   470,   892,   286,   326,\n",
      "            11,  1770,    13,  8759,  2763,   438,  1169,  2994,   284,   943,\n",
      "         17034,   318,   477,   314,   892,   286,   526,   383,  1573,    11,\n",
      "           319,  9074,    13,   536,  5469,   338, 11914,    11, 33096,   663,\n",
      "          4808,  3808,    62,   355,   996,   484,   547, 12548,   287,   281,\n",
      "         13079,   410, 12523,   286, 22353,    13,   843,   340,   373,   407,\n",
      "           691,   262,  9074,    13,   536, 48819,   508, 25722,   276,    13,\n",
      "         11161,   407,   262, 40123, 18113,   544,  9325,   701,    11,   379,\n",
      "           262,   938,   402,  1617,   261, 12917,   905,    11,  5025,   502,\n",
      "           878,   402,   271, 10899,   338,   366, 31640,    12,    67, 20811,\n",
      "             1,   284,   910,    11,   351, 10953,   287,   607,  2951,    25,\n",
      "           366,  1135,  2236,   407,   804,  2402,   663,   588,   757, 13984,\n",
      "           198,   198,  5779, 28112, 10197,   832]])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "second=next(data_iter)\n",
    "print(second)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d12e3f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138,   257,  7026,\n",
      "         15632,   438,  2016,   257,   922,  5891,  1576,   438,   568,   340,\n",
      "           373,   645,  1049,  5975,   284,   502,   284,  3285,   326,    11,\n",
      "           287,   262,  6001,   286,   465, 13476,    11,   339,   550,  5710,\n",
      "           465, 12036,    11,  6405,   257,  5527, 27075,    11,   290,  4920,\n",
      "          2241,   287,   257,  4489,    64,   319,   262, 34686, 41976,    13,\n",
      "           357, 10915,   314,  2138,  1807,   340,   561,   423,   587, 10598,\n",
      "           393, 28537,  2014,   198,   198,     1,   464,  6001,   286,   465,\n",
      "         13476,     1,   438,  5562,   373,   644,   262,  1466,  1444,   340,\n",
      "            13,   314,   460,  3285,  9074,    13, 46606,   536,  5469,   438,\n",
      "         14363,   938,  4842,  1650,   353,   438,  2934,   489,  3255,   465,\n",
      "         48422,   540,   450,    67,  3299,    13,   366,  5189,  1781,   340,\n",
      "           338,  1016,   284,  3758,   262,  1988,   286,   616,  4286,   705,\n",
      "          1014,   510,    26,   475,   314,   836,   470,   892,   286,   326,\n",
      "            11,  1770,    13,  8759,  2763,   438,  1169,  2994,   284,   943,\n",
      "         17034,   318,   477,   314,   892,   286,   526,   383,  1573,    11,\n",
      "           319,  9074,    13,   536,  5469,   338, 11914,    11, 33096,   663,\n",
      "          4808,  3808,    62,   355,   996,   484,   547, 12548,   287,   281,\n",
      "         13079,   410, 12523,   286, 22353,    13,   843,   340,   373,   407,\n",
      "           691,   262,  9074,    13,   536, 48819,   508, 25722,   276,    13,\n",
      "         11161,   407,   262, 40123, 18113,   544,  9325,   701,    11,   379,\n",
      "           262,   938,   402,  1617,   261, 12917,   905,    11,  5025,   502,\n",
      "           878,   402,   271, 10899,   338,   366, 31640,    12,    67, 20811,\n",
      "             1,   284,   910,    11,   351, 10953,   287,   607,  2951,    25,\n",
      "           366,  1135,  2236,   407,   804,  2402,   663,   588,   757, 13984,\n",
      "           198,   198,  5779, 28112, 10197,   832]])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets=next(data_iter)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c88fb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1464,  1807,  3619,   402,   271, 10899,  2138,   257,  7026, 15632,\n",
      "           438,  2016,   257,   922,  5891,  1576,   438,   568,   340,   373,\n",
      "           645,  1049,  5975,   284,   502,   284,  3285,   326,    11,   287,\n",
      "           262,  6001,   286,   465, 13476,    11,   339,   550,  5710,   465,\n",
      "         12036,    11,  6405,   257,  5527, 27075,    11,   290,  4920,  2241,\n",
      "           287,   257,  4489,    64,   319,   262, 34686, 41976,    13,   357,\n",
      "         10915,   314,  2138,  1807,   340,   561,   423,   587, 10598,   393,\n",
      "         28537,  2014,   198,   198,     1,   464,  6001,   286,   465, 13476,\n",
      "             1,   438,  5562,   373,   644,   262,  1466,  1444,   340,    13,\n",
      "           314,   460,  3285,  9074,    13, 46606,   536,  5469,   438, 14363,\n",
      "           938,  4842,  1650,   353,   438,  2934,   489,  3255,   465, 48422,\n",
      "           540,   450,    67,  3299,    13,   366,  5189,  1781,   340,   338,\n",
      "          1016,   284,  3758,   262,  1988,   286,   616,  4286,   705,  1014,\n",
      "           510,    26,   475,   314,   836,   470,   892,   286,   326,    11,\n",
      "          1770,    13,  8759,  2763,   438,  1169,  2994,   284,   943, 17034,\n",
      "           318,   477,   314,   892,   286,   526,   383,  1573,    11,   319,\n",
      "          9074,    13,   536,  5469,   338, 11914,    11, 33096,   663,  4808,\n",
      "          3808,    62,   355,   996,   484,   547, 12548,   287,   281, 13079,\n",
      "           410, 12523,   286, 22353,    13,   843,   340,   373,   407,   691,\n",
      "           262,  9074,    13,   536, 48819,   508, 25722,   276,    13, 11161,\n",
      "           407,   262, 40123, 18113,   544,  9325,   701,    11,   379,   262,\n",
      "           938,   402,  1617,   261, 12917,   905,    11,  5025,   502,   878,\n",
      "           402,   271, 10899,   338,   366, 31640,    12,    67, 20811,     1,\n",
      "           284,   910,    11,   351, 10953,   287,   607,  2951,    25,   366,\n",
      "          1135,  2236,   407,   804,  2402,   663,   588,   757, 13984,   198,\n",
      "           198,  5779, 28112, 10197,   832,   262]])\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfc038b",
   "metadata": {},
   "source": [
    "**converting token IDs (loader) into embeddings** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b03ae3",
   "metadata": {},
   "source": [
    "*for starter let's take a small example for understanding on embeddings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c8646cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let say we have 4 input token ids\n",
    "input_ids=torch.tensor([2,3,5,1])\n",
    "\n",
    "vocab_size=6 \n",
    "output_dim=3 # for GPT-3, output dimension: 12,288\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe2b1b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "embedding_layer=torch.nn.Embedding(vocab_size,output_dim)\n",
    "print(embedding_layer.weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "118a18f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# let's now apply a token ID to get embedding vector\n",
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9f27a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01106f5a",
   "metadata": {},
   "source": [
    "*as we are using BPE for creating token IDs, BPE has a vocab size of 50,257*\n",
    "\n",
    "*for GPT training, OpenAI used output dimension of 12,288*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9938b566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.6403, -1.4213, -2.3757,  ...,  1.3287, -0.4587, -0.6813],\n",
      "        [ 2.1007, -0.7478,  1.1531,  ..., -1.8840,  0.6308,  1.3845],\n",
      "        [-1.5949, -0.4704,  0.2339,  ...,  0.2753,  2.6710,  0.1407],\n",
      "        ...,\n",
      "        [-1.5469, -0.4470, -0.5156,  ...,  0.7707,  0.9065, -1.7708],\n",
      "        [-0.4246,  1.3170,  0.9862,  ...,  0.8581,  0.7659, -1.7296],\n",
      "        [-0.1311,  0.0434, -1.5763,  ..., -2.2314, -0.5979,  0.5495]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "vocab_size=50257\n",
    "output_dim=256 # consider for our purpose\n",
    "token_embedding_layer=torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(token_embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ce11a242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "torch.Size([8, 4])\n",
      "tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "# we'll use our predefined dataloader for a context_size of 4 (max_length)\n",
    "max_lenth=4\n",
    "dataloader=create_dataloader_v1(txt=raw_text,batch_size=8, max_length=max_lenth, stride=max_lenth,shuffle=False)\n",
    "data_iter=iter(dataloader)\n",
    "inputs,targets=next(data_iter)\n",
    "\n",
    "print(inputs)\n",
    "print(inputs.shape)\n",
    "print(targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df6f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n",
      "tensor([[[-1.2006e+00,  6.9948e-01,  1.3829e+00,  ...,  5.4554e-02,\n",
      "           7.9838e-01, -1.7708e-01],\n",
      "         [-4.9637e-01,  1.9622e+00,  1.2301e+00,  ..., -2.9119e+00,\n",
      "          -7.9404e-01, -1.3448e+00],\n",
      "         [ 1.4023e+00, -1.4087e+00,  7.2441e-02,  ...,  1.2578e+00,\n",
      "          -1.9913e+00,  5.7780e-01],\n",
      "         [-1.3574e-01,  2.2363e+00, -4.8063e-01,  ...,  7.6079e-01,\n",
      "          -2.3937e+00,  2.5137e-01]],\n",
      "\n",
      "        [[ 3.0991e-01,  1.6669e+00,  4.0496e-01,  ..., -1.6744e+00,\n",
      "           1.2830e+00,  9.5530e-01],\n",
      "         [-1.2835e+00, -2.3158e-01, -1.1369e-01,  ...,  3.4810e-01,\n",
      "           2.9918e-01,  1.6525e+00],\n",
      "         [-2.0264e+00,  9.7831e-01, -8.8639e-01,  ...,  8.2856e-01,\n",
      "          -4.9504e-01, -1.3272e+00],\n",
      "         [-1.2288e-01, -1.7676e+00,  7.7311e-01,  ..., -2.4732e+00,\n",
      "          -4.9612e-01, -4.1105e-01]],\n",
      "\n",
      "        [[-4.6817e-01, -1.3394e+00, -2.7724e-01,  ..., -8.5541e-01,\n",
      "           1.0628e+00, -6.3077e-01],\n",
      "         [ 1.2375e+00, -1.2663e-01, -3.3823e-01,  ..., -1.5450e+00,\n",
      "          -6.6957e-01,  1.8466e-01],\n",
      "         [-7.3279e-01, -2.2629e-01,  1.9042e+00,  ...,  7.7913e-01,\n",
      "          -1.2307e+00, -5.9616e-01],\n",
      "         [-1.0236e+00,  1.5749e+00, -9.9675e-01,  ..., -1.6685e+00,\n",
      "           9.5785e-02,  7.5168e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.3916e+00, -1.8496e+00,  6.5806e-02,  ..., -8.2898e-01,\n",
      "          -1.5739e+00,  5.4403e-01],\n",
      "         [ 1.0748e+00,  7.6874e-01, -1.8017e-01,  ...,  1.4631e+00,\n",
      "           1.5469e+00, -1.3977e+00],\n",
      "         [-7.5621e-01,  2.5776e-01,  5.2189e-01,  ...,  2.0239e+00,\n",
      "          -6.3806e-01, -2.5054e-01],\n",
      "         [ 2.2883e+00, -2.7105e+00, -2.5729e-01,  ...,  3.6439e-01,\n",
      "          -1.0454e+00,  1.2097e+00]],\n",
      "\n",
      "        [[ 8.1221e-01, -8.1474e-01, -1.4784e+00,  ..., -7.5453e-01,\n",
      "          -2.1648e-01, -3.3949e-01],\n",
      "         [-6.2705e-01,  4.0336e-01, -1.0668e+00,  ..., -1.7357e+00,\n",
      "          -1.3672e+00, -6.9870e-01],\n",
      "         [-6.7061e-01,  3.9357e-01, -6.8570e-02,  ..., -4.1461e-01,\n",
      "           3.6151e-01,  1.4726e-02],\n",
      "         [-1.2075e+00,  1.4200e+00, -1.6731e+00,  ...,  1.7358e+00,\n",
      "          -3.3047e-01,  8.8182e-01]],\n",
      "\n",
      "        [[-6.7061e-01,  3.9357e-01, -6.8570e-02,  ..., -4.1461e-01,\n",
      "           3.6151e-01,  1.4726e-02],\n",
      "         [-3.7387e-01, -1.0625e-01, -1.4882e-01,  ...,  5.4849e-01,\n",
      "          -3.9251e-01, -4.7008e-01],\n",
      "         [-2.2196e-01,  1.1993e-01,  2.4965e-01,  ..., -1.7144e-01,\n",
      "          -1.5803e-03, -3.4898e-01],\n",
      "         [ 5.6118e-01, -1.0668e+00, -5.7652e-01,  ...,  2.3355e-01,\n",
      "           2.4658e+00,  5.9768e-01]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# getting embedding for inputs (8x4)\n",
    "token_embeddings=token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)\n",
    "print(token_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "46ceb86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "# GPT model uses absolute embedding approach\n",
    "\n",
    "context_lenght=max_lenth\n",
    "pos_embedding_layer=torch.nn.Embedding(context_lenght, output_dim)\n",
    "pos_embedding=pos_embedding_layer(torch.arange(context_lenght))\n",
    "print(pos_embedding.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "23e14176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# so, we got token_embedding and positonal embedding,\n",
    "# now we can get the corresponding input_embedding to train an LLM model\n",
    "\n",
    "input_embedding=token_embeddings+pos_embedding\n",
    "print(input_embedding.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12974ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
